{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - TF-IDF Classifier\n",
    "\n",
    "Ваша цель обучить классификатор который будет находить \"токсичные\" комментарии и опубликовать решения на Kaggle [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "\n",
    "В процессе обучения нужно ответить на ***[вопросы](https://docs.google.com/forms/d/e/1FAIpQLSd9mQx8EFpSH6FhCy1M_FmISzy3lhgyyqV3TN0pmtop7slmTA/viewform?usp=sf_link)***\n",
    "\n",
    "Данные можно скачать тут - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import hstack\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize.toktok  import ToktokTokenizer\n",
    "from gensim.utils import tokenize\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('../data/01/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../data/01/test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стадартными подходами для анализа текста являются [Bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) и его модификация [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "Они реалзованны в `sklearn` в виде [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) и [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "Более подробней про них можно посмотреть [тут](https://github.com/udsclub/workshop/blob/master/notebooks/UDS-workshop-feature-extraction-and-engineering.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какое слово встречается чаще всего в объединенном train и test датасете? *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_in_one = \" \".join(all_text.values)\n",
    "tokens = tokenize(all_text_in_one.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tokens).value_counts().argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    #tokens = ToktokTokenizer().tokenize(text.lower()) - гірше\n",
    "    #tokens = TweetTokenizer().tokenize(text.lower()) - гірше\n",
    "    tokens = tokenize(text.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Попробуйте разные Vectorizer и разные размеры n-gramm, стоп-слова, обрезку редких слов, обрезку слишком частых слов\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                                  tokenizer=tokenizer,\n",
    "                                  max_features=20000, \n",
    "                                  norm='l2',\n",
    "                                  smooth_idf=False,\n",
    "                                  sublinear_tf=True,\n",
    "                                  strip_accents='unicode',\n",
    "                                  min_df=2, max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 49s, sys: 324 ms, total: 2min 50s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                  strip_accents='unicode',\n",
    "                                  norm = 'l2',\n",
    "                                  analyzer='char',\n",
    "                                  ngram_range=(2, 5),\n",
    "                                  smooth_idf=False,\n",
    "                                  max_features=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 2s, sys: 13.7 s, total: 9min 16s\n",
      "Wall time: 10min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.52 s, sys: 2.85 s, total: 6.37 s\n",
      "Wall time: 6.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации будем использовать логистическую регрессию [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем тренировать по одному классификатору на каждый класс. \n",
    "\n",
    "Что бы провалидировать качество модели воспользуемся функцией [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9794433352823435\n",
      "CV score for class severe_toxic is 0.988447230370659\n",
      "CV score for class obscene is 0.9906364594646182\n",
      "CV score for class threat is 0.9898797548530512\n",
      "CV score for class insult is 0.9833815847019892\n",
      "CV score for class identity_hate is 0.9832311610565768\n",
      "Total score is 0.9858365876215397\n",
      "CPU times: user 8min 33s, sys: 27.1 s, total: 9min\n",
      "Wall time: 9min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#kaggle 97.98\n",
    "scores= []\n",
    "c_s = [1.5, 1, 1.5, 1.8, 1, 1]\n",
    "\n",
    "for c, class_name in zip(c_s, class_names):\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(solver='sag', C=c)\n",
    "    #grid = GridSearchCV(param_grid=parameters, estimator=classifier, scoring='roc_auc', n_jobs=-1)\n",
    "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, scoring='roc_auc'))\n",
    "    #grid.fit(train_features, train_target)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "    scores.append(cv_score)\n",
    "    #c_s.append(grid.best_params_)\n",
    "\n",
    "print('Total score is {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте подобрать лучшие параметры для `word_vectorizer` и `classifier` оптимизируя метрику [ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опубликуйте лучшие решение на [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({'id': test['id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 18s, sys: 11.9 s, total: 5min 30s\n",
      "Wall time: 5min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "c_s = [1.5, 1, 1.5, 1.8, 1, 1]\n",
    "for c, class_name in zip(c_s, class_names):\n",
    "    classifier = LogisticRegression(solver='sag', n_jobs=-1, C=c)\n",
    "    train_target = train[class_name]\n",
    "    classifier.fit(train_features, train_target)\n",
    "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.999931</td>\n",
       "      <td>0.234588</td>\n",
       "      <td>0.999732</td>\n",
       "      <td>0.089795</td>\n",
       "      <td>0.984243</td>\n",
       "      <td>0.307368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>0.002385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.008445</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.003073</td>\n",
       "      <td>0.001347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>0.000585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.012878</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.005104</td>\n",
       "      <td>0.000995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.999931      0.234588  0.999732  0.089795  0.984243   \n",
       "1  0000247867823ef7  0.003666      0.001656  0.001487  0.000150  0.004741   \n",
       "2  00013b17ad220c46  0.008445      0.002868  0.004800  0.000364  0.003073   \n",
       "3  00017563c3f7919a  0.002142      0.001437  0.001673  0.000546  0.003105   \n",
       "4  00017695ad8997eb  0.012878      0.001502  0.003669  0.000642  0.005104   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.307368  \n",
       "1       0.002385  \n",
       "2       0.001347  \n",
       "3       0.000585  \n",
       "4       0.000995  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some of 1000 tries that I decided to leave for my own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9794002214479471\n",
      "CV score for class severe_toxic is 0.9883912317939827\n",
      "CV score for class obscene is 0.9907130943933756\n",
      "CV score for class threat is 0.9897664108800015\n",
      "CV score for class insult is 0.983198022464808\n",
      "CV score for class identity_hate is 0.9832588955950823\n",
      "Total score is 0.9857879794291996\n"
     ]
    }
   ],
   "source": [
    "#kaggle 97.95, гірше\n",
    "def tokenizer(text):\n",
    "    #tokens = ToktokTokenizer().tokenize(text.lower())\n",
    "    #tokens = TweetTokenizer().tokenize(text.lower())\n",
    "    tokens = tokenize(text.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmas\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                                  tokenizer=tokenizer,\n",
    "                                  max_features=15000, \n",
    "                                  norm='l2',\n",
    "                                  smooth_idf=False,\n",
    "                                  sublinear_tf=True,\n",
    "                                  strip_accents='unicode')\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                  strip_accents='unicode',\n",
    "                                  norm = 'l2',\n",
    "                                  analyzer='char',\n",
    "                                  ngram_range=(2, 4),\n",
    "                                  smooth_idf=False,\n",
    "                                  max_features=50000,\n",
    "                                  stop_words=STOPWORDS)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n",
    "scores= []\n",
    "c_s = [1.5, 1, 1.5, 1.8, 1, 1]\n",
    "\n",
    "for c, class_name in zip(c_s, class_names):\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(solver='lbfgs', C=c)\n",
    "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, scoring='roc_auc'))\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "    scores.append(cv_score)\n",
    "\n",
    "print('Total score is {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9793825513525802\n",
      "CV score for class severe_toxic is 0.9884973386970457\n",
      "CV score for class obscene is 0.9907118909144917\n",
      "CV score for class threat is 0.9897773714502339\n",
      "CV score for class insult is 0.9832516759406967\n",
      "CV score for class identity_hate is 0.9833925208704678\n",
      "Total score is 0.9858355582042527\n"
     ]
    }
   ],
   "source": [
    "#kaggle 97.96 гірше\n",
    "def tokenizer(text):\n",
    "    #tokens = ToktokTokenizer().tokenize(text.lower())\n",
    "    #tokens = TweetTokenizer().tokenize(text.lower())\n",
    "    tokens = tokenize(text.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmas\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                                  tokenizer=tokenizer,\n",
    "                                  max_features=15000, \n",
    "                                  norm='l2',\n",
    "                                  smooth_idf=True,\n",
    "                                  sublinear_tf=True,\n",
    "                                  strip_accents='unicode', \n",
    "                                  max_df=0.95)\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                  strip_accents='unicode',\n",
    "                                  norm = 'l2',\n",
    "                                  analyzer='char',\n",
    "                                  ngram_range=(2, 4),\n",
    "                                  smooth_idf=True,\n",
    "                                  max_features=30000,\n",
    "                                  stop_words = STOPWORDS)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n",
    "scores= []\n",
    "c_s = [1.5, 1, 1.5, 1.8, 1, 1]\n",
    "\n",
    "for c, class_name in zip(c_s, class_names):\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(solver='lbfgs', C=c)\n",
    "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, scoring='roc_auc'))\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "    scores.append(cv_score)\n",
    "\n",
    "print('Total score is {}'.format(np.mean(scores)))\n",
    "\n",
    "\n",
    "c_s = [1.5, 1, 1.5, 1.8, 1, 1]\n",
    "for c, class_name in zip(c_s, class_names):\n",
    "    classifier = LogisticRegression(solver='lbfgs', n_jobs=-1, C=c)\n",
    "    train_target = train[class_name]\n",
    "    classifier.fit(train_features, train_target)\n",
    "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "    \n",
    "submission.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#kaggle 97.93\n",
    "def tokenizer(text):\n",
    "    #tokens = ToktokTokenizer().tokenize(text.lower())\n",
    "    #tokens = TweetTokenizer().tokenize(text.lower())\n",
    "    tokens = tokenize(text.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = [stemmer.stem(l) for l in lemmas]\n",
    "    return stems\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                                  tokenizer=tokenizer,\n",
    "                                  max_features=15000, \n",
    "                                  norm='l2',\n",
    "                                  smooth_idf=False,\n",
    "                                  sublinear_tf=True,\n",
    "                                  strip_accents='unicode')\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                  strip_accents='unicode',\n",
    "                                  norm = 'l2',\n",
    "                                  analyzer='char',\n",
    "                                  ngram_range=(2, 5),\n",
    "                                  smooth_idf=False,\n",
    "                                  max_features=50000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "print(\"1\")\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "print(\"1\")\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n",
    "print(\"1\")\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "c_s = [1.5, 1, 1.5, 1.8, 1, 1]\n",
    "for c, class_name in zip(c_s, class_names):\n",
    "    classifier = LogisticRegression(solver='sag', n_jobs=-1, C=c)\n",
    "    train_target = train[class_name]\n",
    "    classifier.fit(train_features, train_target)\n",
    "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "    \n",
    "submission.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    #tokens = ToktokTokenizer().tokenize(text.lower())\n",
    "    #tokens = TweetTokenizer().tokenize(text.lower())\n",
    "    tokens = tokenize(text.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmas\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                                  tokenizer=tokenizer,\n",
    "                                  max_features=15000, \n",
    "                                  norm='l2',\n",
    "                                  smooth_idf=True,\n",
    "                                  sublinear_tf=True,\n",
    "                                  strip_accents='unicode',\n",
    "                                  min_df=2, max_df=0.7)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "print(\"1\")\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n",
    "print(\"1\")\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "c_s = [1.5, 1, 1.5, 1.8, 1, 1]\n",
    "for c, class_name in zip(c_s, class_names):\n",
    "    classifier = LogisticRegression(solver='sag', n_jobs=-1, C=c)\n",
    "    train_target = train[class_name]\n",
    "    classifier.fit(train_features, train_target)\n",
    "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "    \n",
    "submission.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
