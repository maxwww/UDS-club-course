{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 - Wikipedia Web Traffic Time Series\n",
    "\n",
    "У вас есть данные по посещению 1000 страниц  Википедии из разных стран и разных девайсов ( \\*  * данные взяты из [Kaggle соревнования](https://www.kaggle.com/c/web-traffic-time-series-forecasting)* )\n",
    "\n",
    "*wikipedia_train* и *wikipedia_test* - содержат данные о трафике. Это файлы csv, где каждая строка соответствует определенной статье, и каждый столбец соответствует конкретной дате. В некоторых записях отсутствуют данные. Названия страниц содержат проект Википедии (например, en.wikipedia.org), тип доступа (например, desktop) и тип агента (например, spider). Другими словами, каждое имя статьи имеет следующий формат: «name_project_access_agent» (например, «AKB48_zh.wikipedia.org_all-access_spider»).\n",
    "\n",
    "Вам нужно ответить на [вопросы](https://docs.google.com/forms/d/e/1FAIpQLSfDjWeeZJw5EvmKn1x_6b9xicjn7ed3MF0rbNm4Cmwr7psSkQ/viewform?usp=sf_link) и попробовать сделать самую простую модель которая сможет предсказывать будущие посещения. \n",
    "\n",
    "Вот примеры временных рядов посещаемости страниц Википедии (*синие* - обучающая выборка, *зеленые* - предсказания модели победителя соревнования на Kaggle, *оранжевые* - реальные значения):\n",
    "![Wikipedia Web Traffic Time Series](https://image.ibb.co/cUpEJa/predictions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/wikipedia_train.csv\")\n",
    "test = pd.read_csv(\"../data/wikipedia_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_language(page):\n",
    "    res = re.search('[a-z][a-z].wikipedia.org',page)\n",
    "    if res:\n",
    "        return res.group(0)[0:2]\n",
    "    return 'na'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно преобразовать `train` данные в следующий формат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = train.melt(id_vars=['Page'], value_name='Visits', var_name='Date')\n",
    "test_df = test.melt(id_vars=['Page'], value_name='Visits', var_name='Date')\n",
    "predictions = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом у вас каждая сточка содержит набор фич (`Page`, `date`) и целевую переменную (`Visits`). Преобразовать данные в такой формат поможет функция `pd.melt()` (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Date</th>\n",
       "      <th>Visits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15._November_de.wikipedia.org_desktop_all-agents</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012_(film)_fr.wikipedia.org_all-access_spider</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016_FIFA_U-20女子ワールドカップ_ja.wikipedia.org_all-a...</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016_UEFA_Europa_League_Final_en.wikipedia.org...</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016_in_video_gaming_en.wikipedia.org_all-acce...</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Page        Date  Visits\n",
       "0   15._November_de.wikipedia.org_desktop_all-agents  2015-07-01    32.0\n",
       "1     2012_(film)_fr.wikipedia.org_all-access_spider  2015-07-01     2.0\n",
       "2  2016_FIFA_U-20女子ワールドカップ_ja.wikipedia.org_all-a...  2015-07-01     1.0\n",
       "3  2016_UEFA_Europa_League_Final_en.wikipedia.org...  2015-07-01     3.0\n",
       "4  2016_in_video_gaming_en.wikipedia.org_all-acce...  2015-07-01    24.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценивать качество предсказаний мы будем с помощью [SMAPE](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pandas_smape(df):\n",
    "    df.fillna(0, inplace=True)\n",
    "    df[\"SMAPE\"] = 200 * np.abs(df[\"Visits\"] - df[\"pred_Visits\"]) / (df[\"Visits\"] + df[\"pred_Visits\"])\n",
    "    df[\"SMAPE\"].fillna(0, inplace=True)\n",
    "    return np.mean(df[\"SMAPE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сколько страниц из русской Википедии в датасете?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество страниц русской википедии: 102\n"
     ]
    }
   ],
   "source": [
    "train['language'] =train[\"Page\"].apply(get_language)\n",
    "print(f\"Количество страниц русской википедии: {train.language.value_counts()['ru']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какая самая популярная страница русской Википедии (в среднем)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самая популярная страница русской википедии: Facebook_ru.wikipedia.org_desktop_all-agents\n"
     ]
    }
   ],
   "source": [
    "train['average_visits'] = train.mean(axis=1)\n",
    "top = train[train.language == 'ru'].sort_values('average_visits', ascending=False).iloc[0,0]\n",
    "print(f\"Самая популярная страница русской википедии: {top}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.drop(train.columns[-2:], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last day baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно сделать прогноз на основе посещений в последний известный нам день из train (продублировать значение для каждого дня в test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.988221062619054"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#method1\n",
    "helper = train_df.dropna().sort_values(['Page', 'Date'])\\\n",
    ".drop_duplicates(['Page'], keep='last').set_index('Page').transpose()\n",
    "predictions['pred_Visits'] = predictions.Page.apply(lambda x: helper[x].Visits)\n",
    "pandas_smape(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.988221062619054"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#method2\n",
    "page_v = {}\n",
    "p_d = train.set_index('Page').T\n",
    "for page in p_d.columns:\n",
    "    last_valid = p_d[page].last_valid_index()\n",
    "    page_v[page] = p_d.loc[last_valid][page]\n",
    "    \n",
    "predictions['pred_Visits'] = predictions.Page.apply(lambda x: page_v[x])\n",
    "pandas_smape(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.16127748085736"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#method3\n",
    "helper = train.drop(train.columns[1:-1], axis=1).set_index('Page').transpose()\n",
    "predictions['pred_Visits'] = predictions.Page.apply(lambda x: helper[x])\n",
    "pandas_smape(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно сделать прогноз на основе медианы за последние **30** дней из `train`. \n",
    "\n",
    "А затем улучшить предсказания используя информацию выходной это или нет (воспользуйтесь функцией [dayofweek](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.dayofweek.html) ) и разные окна для подсчета медианы (7 дней, 60 дней и тд)\n",
    "\n",
    "Вам поможет функция `pd.groupby()` (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Медиана за 30 дней**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.Date = pd.to_datetime(train_df.Date)\n",
    "train_df['dayOfWeek'] = train_df.Date.apply(lambda x: x.dayofweek)\n",
    "train_df['weekend'] = train_df.dayOfWeek.apply(lambda x: 1 if x in (5, 6) else 0)\n",
    "predictions.Date = pd.to_datetime(predictions.Date)\n",
    "predictions['dayOfWeek'] = predictions.Date.apply(lambda x: x.dayofweek)\n",
    "predictions['weekend'] = predictions.dayOfWeek.apply(lambda x: 1 if x in (5, 6) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.46588329336902"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper = train_df[train_df.Date > '2016-08-01'].groupby('Page')['Visits'].median()\n",
    "predictions['pred_Visits'] = predictions.Page.apply(lambda x: helper[x])\n",
    "pandas_smape(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.89889768202033"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#не просили заміняти, це не треба\n",
    "helper = train_df[train_df.Date > '2016-08-01'].fillna(0).groupby('Page')['Visits'].median()\n",
    "predictions['pred_Visits'] = predictions.Page.apply(lambda x: helper[x])\n",
    "pandas_smape(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Медиана за 30 дней и выходные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.360885003115996"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper = train_df[train_df.Date > '2016-08-01'].groupby(['Page', 'weekend']).median().transpose()\n",
    "predictions['pred_Visits'] = predictions[['Page','weekend']].apply(lambda x: helper[x[0]][x[1]].Visits, axis=1)\n",
    "pandas_smape(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Лучшая модель!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yulia/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/numpy/lib/function_base.py:4016: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48.58044749458924"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper = train.fillna(0)\n",
    "i=0\n",
    "windows = [-1,-2,-3,-5,-8,-13,-21,-34,-55,-89,1,1,2,3,5,8,12,14,21,34,55,89,100,110, 200,244,360]\n",
    "for w in windows:\n",
    "    i+=1\n",
    "    helper['pred_Visits' + str(i)] = train[train.columns[-w:]].agg(np.nanmedian, axis=1)\n",
    "\n",
    "helper['pred_Visits'] = helper[helper.columns[-len(windows):]].median(axis=1)\n",
    "helper['pred_Visits'] = helper['pred_Visits'].apply(lambda x: round(x))\n",
    "helper = helper.drop(helper.columns[1:-1], axis = 1).set_index('Page').transpose()\n",
    "predictions['pred_Visits'] = predictions.Page.apply(lambda x: helper[x].pred_Visits)\n",
    "pandas_smape(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weekends_workends(train):\n",
    "    dates = train.set_index('Page').columns[:-2]\n",
    "    dates = dates.astype('datetime64[ns]')\n",
    "    weekends = []\n",
    "    workends = []\n",
    "    for date in dates :\n",
    "        if date.dayofweek == 5 or date.dayofweek==6:\n",
    "            weekends.append(date.date())\n",
    "        else:\n",
    "            workends.append(date.date())\n",
    "    weekends = [weekend.strftime('%Y-%m-%d') for weekend in weekends]\n",
    "    workends = [workend.strftime('%Y-%m-%d') for workend in workends]\n",
    "    return weekends, workends\n",
    "\n",
    "def get_days(train):\n",
    "    dates = train.set_index('Page').columns[:-2]\n",
    "    dates = dates.astype('datetime64[ns]')\n",
    "    mn = []\n",
    "    tu = []\n",
    "    wd = []\n",
    "    th = []\n",
    "    fr = []\n",
    "    st = []\n",
    "    sn = []\n",
    "    for date in dates :\n",
    "        if date.dayofweek == 0:\n",
    "            mn.append(date.date())\n",
    "        elif date.dayofweek == 1:\n",
    "            tu.append(date.date())\n",
    "        elif date.dayofweek == 2:\n",
    "            wd.append(date.date()) \n",
    "        elif date.dayofweek == 3:\n",
    "            th.append(date.date())\n",
    "        elif date.dayofweek == 4:\n",
    "            fr.append(date.date())\n",
    "        elif date.dayofweek == 5:\n",
    "            st.append(date.date())\n",
    "        else:\n",
    "            sn.append(date.date())\n",
    "    mn = [weekend.strftime('%Y-%m-%d') for weekend in mn]\n",
    "    tu = [weekend.strftime('%Y-%m-%d') for weekend in tu]\n",
    "    wd = [weekend.strftime('%Y-%m-%d') for weekend in wd]\n",
    "    th = [weekend.strftime('%Y-%m-%d') for weekend in th]\n",
    "    fr = [weekend.strftime('%Y-%m-%d') for weekend in fr]\n",
    "    st = [weekend.strftime('%Y-%m-%d') for weekend in st]\n",
    "    sn = [weekend.strftime('%Y-%m-%d') for weekend in sn]\n",
    "    return mn, tu,wd,th,fr,st,sn\n",
    "\n",
    "def split_df(df):\n",
    "    weekends, workends = get_weekends_workends(df)\n",
    "    columns_weekends = np.append('Page', weekends)\n",
    "    columns_workends = np.append('Page', workends)\n",
    "    return df[columns_weekends], df[columns_workends]\n",
    "\n",
    "def split_df2(df):\n",
    "    m,t,w,t1,f,s,s1 = get_days(df)\n",
    "    columns_m = np.append('Page', m)\n",
    "    columns_t = np.append('Page', t)\n",
    "    columns_w = np.append('Page', w)\n",
    "    columns_t1 = np.append('Page', t1)\n",
    "    columns_f = np.append('Page', f)\n",
    "    columns_s = np.append('Page', s)\n",
    "    columns_s1 = np.append('Page', s1)\n",
    "    return df[columns_m], df[columns_t], df[columns_w], df[columns_t1], df[columns_f], df[columns_s], df[columns_s1]\n",
    "\n",
    "def predict(train, test):\n",
    "    train.fillna(0, inplace=True)\n",
    "    helper = train.copy()\n",
    "    test_df = test.melt(id_vars=['Page'], value_name='Visits', var_name='Date')\n",
    "    predictions = test_df.copy()\n",
    "    i=0\n",
    "    windows = [-1,-2,-3,-13,-21,-34,-55,-89,-100, -110, 1,1,2,3,5,8,12,14,21,34,55,89,100,110]\n",
    "    for w in windows:\n",
    "        i+=1\n",
    "        helper['pred_Visits' + str(i)] = train[train.columns[-w:]].agg(np.nanmedian, axis=1)\n",
    "    helper['pred_Visits'] = helper[helper.columns[-len(windows):]].median(axis=1)\n",
    "    helper['pred_Visits'] = helper['pred_Visits'].apply(lambda x: round(x))\n",
    "    helper =helper.drop(helper.columns[1:-1], axis = 1).set_index('Page').transpose()\n",
    "    predictions['pred_Visits'] = predictions.Page.apply(lambda x: helper[x].pred_Visits)\n",
    "    return predictions\n",
    "\n",
    "def predict2(train, test):\n",
    "    train.fillna(0, inplace=True)\n",
    "    helper = train.copy()\n",
    "    test_df = test.melt(id_vars=['Page'], value_name='Visits', var_name='Date')\n",
    "    predictions = test_df.copy()\n",
    "    i=0\n",
    "    windows = [-1,-2,-3,-8,-12,-15,-21,-34,-55,-60,1,1,1,2,2,4,6,7,14,35,56,60]\n",
    "    for w in windows:\n",
    "        i+=1\n",
    "        helper['pred_Visits' + str(i)] = train[train.columns[-w:]].agg(np.nanmedian, axis=1)\n",
    "    helper['pred_Visits'] = helper[helper.columns[-len(windows):]].median(axis=1)\n",
    "    helper['pred_Visits'] = helper['pred_Visits'].apply(lambda x: round(x))\n",
    "    helper =helper.drop(helper.columns[1:-1], axis = 1).set_index('Page').transpose()\n",
    "    predictions['pred_Visits'] = predictions.Page.apply(lambda x: helper[x].pred_Visits)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yulia/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/pandas/core/frame.py:2754: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46.10240333664849"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_m, train_t, train_w, train_t1, train_f, train_s, train_s1 = split_df2(train)\n",
    "test_m, test_t, test_w, test_t1, test_f, test_s, test_s1 = split_df2(test)\n",
    "\n",
    "t_m= predict2(train_m, test_m)\n",
    "t_t= predict2(train_t, test_t)\n",
    "t_w= predict2(train_w, test_w)\n",
    "t_t1= predict2(train_t1, test_t1)\n",
    "t_f= predict2(train_f, test_f)\n",
    "t_s= predict2(train_s, test_s)\n",
    "t_s1= predict2(train_s1, test_s1)\n",
    "n_d = pd.concat([t_m,t_t,t_w,t_t1,t_f,t_s,t_s1], axis=0)\n",
    "pandas_smape(n_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yulia/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/pandas/core/frame.py:2754: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46.19476571088786"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weekends, train_workends = split_df(train)\n",
    "test_weekends, test_workends = split_df(test)\n",
    "\n",
    "t_wrk = predict(train_workends, test_workends)\n",
    "t_week = predict(train_weekends, test_weekends)\n",
    "n_d = pd.concat([t_week, t_wrk], axis=0)\n",
    "pandas_smape(n_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Ця модель краще rolling window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
